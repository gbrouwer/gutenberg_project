{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as pd\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from gensim import corpora, models\n",
    "from nltk import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all lines in a doc and cleaning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtxt(filename):\n",
    "    \n",
    "    lines = []\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if (len(line) > 0):\n",
    "                line = re.sub(r'[^a-zA-Z0-9_]+', ' ', line)\n",
    "                line = line.lower()\n",
    "                lines.append(line)\n",
    "    lines = ' '.join(lines)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv('../data/database.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict to English and load a subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "database = database[database['language'] == 'english']\n",
    "subdata = database.sample(250)\n",
    "for filename in subdata['filename'].values:\n",
    "    doc = readtxt(filename)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Preprocess helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    result = []\n",
    "    stemmer = PorterStemmer() \n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            #token = WordNetLemmatizer().lemmatize(token, pos='v')\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:25<00:00, 10.76it/s]\n"
     ]
    }
   ],
   "source": [
    "documents_processed = []\n",
    "for document in tqdm(documents):\n",
    "    document = preprocess(document)\n",
    "    documents_processed.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(documents_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc 2 Bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents_processed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowdoc = bow_corpus[0]\n",
    "for i in range(len(bowdoc)):\n",
    "    pass\n",
    "    #print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bowdoc[i][0], dictionary[bowdoc[i][0]], bowdoc[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-iDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.001*\"shakespeare\" + 0.001*\"titanic\" + 0.000*\"clara\" + 0.000*\"wagner\" + 0.000*\"cask\" + 0.000*\"wharf\" + 0.000*\"casks\" + 0.000*\"ellis\" + 0.000*\"agin\" + 0.000*\"marie\"\n",
      "Topic: 1 Word: 0.001*\"maurice\" + 0.001*\"polly\" + 0.001*\"sandy\" + 0.001*\"ginger\" + 0.001*\"jackson\" + 0.000*\"monsieur\" + 0.000*\"crane\" + 0.000*\"witches\" + 0.000*\"margaret\" + 0.000*\"israel\"\n",
      "Topic: 2 Word: 0.001*\"jean\" + 0.001*\"thou\" + 0.000*\"writ\" + 0.000*\"foster\" + 0.000*\"thee\" + 0.000*\"maurice\" + 0.000*\"maid\" + 0.000*\"walter\" + 0.000*\"ships\" + 0.000*\"crows\"\n",
      "Topic: 3 Word: 0.001*\"solomon\" + 0.001*\"metaphor\" + 0.000*\"healing\" + 0.000*\"eliza\" + 0.000*\"negroes\" + 0.000*\"missionary\" + 0.000*\"punch\" + 0.000*\"jimmy\" + 0.000*\"flora\" + 0.000*\"palmer\"\n",
      "Topic: 4 Word: 0.001*\"morris\" + 0.001*\"footnote\" + 0.000*\"feller\" + 0.000*\"violin\" + 0.000*\"madame\" + 0.000*\"leon\" + 0.000*\"coventry\" + 0.000*\"carter\" + 0.000*\"jesus\" + 0.000*\"lavender\"\n",
      "Topic: 5 Word: 0.001*\"thou\" + 0.001*\"thee\" + 0.001*\"aunt\" + 0.001*\"christ\" + 0.001*\"dick\" + 0.001*\"anne\" + 0.000*\"colonel\" + 0.000*\"philip\" + 0.000*\"gordon\" + 0.000*\"jones\"\n",
      "Topic: 6 Word: 0.000*\"fisher\" + 0.000*\"maud\" + 0.000*\"emma\" + 0.000*\"countess\" + 0.000*\"pope\" + 0.000*\"paul\" + 0.000*\"oscar\" + 0.000*\"jack\" + 0.000*\"miners\" + 0.000*\"sheriff\"\n",
      "Topic: 7 Word: 0.001*\"jerusalem\" + 0.001*\"demure\" + 0.001*\"socrates\" + 0.001*\"rocket\" + 0.001*\"amos\" + 0.001*\"signor\" + 0.001*\"austin\" + 0.001*\"craven\" + 0.000*\"unto\" + 0.000*\"planet\"\n",
      "Topic: 8 Word: 0.001*\"alice\" + 0.001*\"agnes\" + 0.001*\"canada\" + 0.001*\"germans\" + 0.001*\"ashore\" + 0.001*\"british\" + 0.000*\"hans\" + 0.000*\"taxation\" + 0.000*\"baltimore\" + 0.000*\"prompt\"\n",
      "Topic: 9 Word: 0.001*\"wits\" + 0.000*\"frog\" + 0.000*\"froth\" + 0.000*\"artery\" + 0.000*\"nick\" + 0.000*\"matilda\" + 0.000*\"celtic\" + 0.000*\"julia\" + 0.000*\"augustus\" + 0.000*\"dans\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
